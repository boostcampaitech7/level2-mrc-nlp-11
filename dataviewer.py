import streamlit as st
import numpy as np
from datasets import load_from_disk
import json
import hydra
from module.data import *
import pandas as pd


# í™”ë©´ ë ˆì´ì•„ì›ƒ ì„¤ì •
st.set_page_config(layout="wide", page_title="SEVEN ELEVEN MRC Data Viewer V1.0.0")


@st.cache_data
def load_data(_config):
    """
    ì§ˆë¬¸-ë¬¸ì„œ(-ì •ë‹µ) í˜ì–´ ë°ì´í„°ì…‹ì„ Dataframe í˜•ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    Args:
        _config: í…œí”Œë¦¿ì˜ config/mrc.yaml íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš° ì§€ìš°ê±°ë‚˜ ì£¼ì„ì²˜ë¦¬í•˜ê³  ì‚¬ìš©í•˜ë ¤ëŠ” ë°ì´í„° ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.
    Returns:
        dataset: ì§ˆë¬¸-ë¬¸ì„œ(-ì •ë‹µ) í˜ì–´ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
    """

    # data_path = (
    #     os.path.dirname(os.path.abspath(__file__)) + f"/data/train_dataset/train"
    # )  # ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„°ì˜ train_dataset/train
    data_path = (
        os.path.dirname(os.path.abspath(__file__)) + f"/data/train_dataset/validation"
    )  # ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„°ì˜ train_dataset/validation

    # # configì— ì„¤ì •í•œ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
    # # **ë¯¸êµ¬í˜„**
    # dataset_list = get_dataset_list(_config.data.dataset_name)

    if os.path.exists(data_path):
        dataset = load_from_disk(data_path)
        dataset = dataset.to_pandas()
        return dataset
    else:
        print("âš ï¸ì§€ì •í•œ ê²½ë¡œì— ë°ì´í„°ì…‹ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None


@st.cache_data
def load_predictions(_config):
    """
    ì§ˆë¬¸ì— ëŒ€í•œ nbest ì˜ˆì¸¡ ë°ì´í„°ë¥¼ Object í˜•ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    Args:
        _config: í…œí”Œë¦¿ì˜ config/mrc.yaml íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš° ì§€ìš°ê±°ë‚˜ ì£¼ì„ì²˜ë¦¬í•˜ê³  ì‚¬ìš©í•˜ë ¤ëŠ” ë°ì´í„° ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.
    Returns:
        predictions: ì§ˆë¬¸ì— ëŒ€í•œ nbest ì˜ˆì¸¡ ë°ì´í„°
    """
    # configì— ì„¤ì •í•œ output dir ê²½ë¡œì˜ nbest prediction
    predictions_path = f"{_config.train.output_dir}/eval_nbest_predictions.json"
    # prediction_path = os.path.dirname(os.path.abspath(__file__)) + "/outputs/eval_nbest_predictions.json"

    if os.path.exists(predictions_path):
        with open(predictions_path, "r", encoding="utf-8") as f:
            predictions = json.load(f)
        return predictions
    else:
        print("âš ï¸ì§€ì •í•œ ê²½ë¡œì— nbest prediction íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None


@st.cache_data
def load_tokenized_samples(_config):
    """
    ì§ˆë¬¸-ë¬¸ì„œ ì‹œí€€ìŠ¤ì˜ í† í¬ë‚˜ì´ì§• ê²°ê³¼ ë°ì´í„°ë¥¼ Object í˜•ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    í† í° ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì›í•˜ëŠ” ë°ì´í„°ì— ëŒ€í•´ utils/save_tokenized_samples.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.
    Args:
        _config: í…œí”Œë¦¿ì˜ config/mrc.yaml íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš° ì§€ìš°ê±°ë‚˜ ì£¼ì„ì²˜ë¦¬í•˜ê³  ì‚¬ìš©í•˜ë ¤ëŠ” ë°ì´í„° ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.
    Returns:
        tokenized_samples: ì§ˆë¬¸-ë¬¸ì„œ ì‹œí€€ìŠ¤ì˜ í† í¬ë‚˜ì´ì§• ê²°ê³¼ ë°ì´í„°
    """
    tokenized_samples_path = f"{_config.train.output_dir}/eval_tokenized_samples.json"
    # tokenized_samples_path = os.path.dirname(os.path.abspath(__file__)) + "/outputs/tokenized_samples.json"

    if os.path.exists(tokenized_samples_path):
        with open(tokenized_samples_path, "r", encoding="utf-8") as f:
            tokenized_samples = json.load(f)
        return tokenized_samples
    else:
        print("âš ï¸ì§€ì •í•œ ê²½ë¡œì— tokenized sample íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None


@st.cache_data
def load_wiki():
    wiki_path = (
        os.path.dirname(os.path.abspath(__file__)) + "/data/wikipedia_documents.json"
    )
    if os.path.exists(wiki_path):
        with open(wiki_path, "r", encoding="utf-8") as f:
            wiki_data = json.load(f)
        # text ê°’ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°í•˜ì—¬ ë°˜í™˜
        wiki_unique = {}
        for v in wiki_data.values():
            if v["text"] not in wiki_unique.keys():
                wiki_unique[v["text"]] = v
        return list(wiki_unique.values())
    else:
        print("âš ï¸ì§€ì •í•œ ê²½ë¡œì— wiki documents íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None


@st.cache_data
def load_tfidf_info(_config):
    tfidf_info_path = f"{_config.train.output_dir}/wiki_tfidf_info.json"

    if os.path.exists(tfidf_info_path):
        with open(tfidf_info_path, "r", encoding="utf-8") as f:
            tfidf_info = json.load(f)
        return tfidf_info
    else:
        print("âš ï¸ì§€ì •í•œ ê²½ë¡œì— tfidf info íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None


@st.cache_data
def load_retrieved_documents(_config):
    test_dataset_path = (
        os.path.dirname(os.path.abspath(__file__))
        + f"/data/default/test_dataset/validation"
    )
    retrieved_documents_path = f"{_config.train.output_dir}/retrieved_documents.json"

    if os.path.exists(test_dataset_path):
        test_dataset = load_from_disk(test_dataset_path)
        test_dataset = test_dataset.to_pandas()
    else:
        print("âš ï¸ì§€ì •í•œ ê²½ë¡œì— ë°ì´í„°ì…‹ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        test_dataset = None

    if os.path.exists(retrieved_documents_path):
        with open(retrieved_documents_path, "r", encoding="utf-8") as f:
            retrieved_documents = json.load(f)
    else:
        print("âš ï¸ì§€ì •í•œ ê²½ë¡œì— retrieved documents íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        retrieved_documents = None

    return test_dataset, retrieved_documents


def view_QA(question_id, data, prediction):
    """
    ì„ íƒí•œ ì§ˆë¬¸-ë¬¸ì„œ í˜ì–´ë¥¼ ë³´ê¸° ì¢‹ê²Œ í™”ë©´ì— í‘œì‹œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    Args:
        question_id: ì„ íƒí•œ ì§ˆë¬¸ id
        data: question_idì— ëŒ€ì‘í•˜ëŠ” ì§ˆë¬¸-ë¬¸ì„œ í˜ì–´ ë°ì´í„°
        prediction: question_idì— ëŒ€ì‘í•˜ëŠ” ì˜ˆì¸¡ ë°ì´í„°(ì—†ìœ¼ë©´ í‘œì‹œ ì•ˆ ë¨)
    Returns:
        None
    """
    if not data.empty:
        # ì§ˆë¬¸, ë‹µë³€, ê·¸ë¦¬ê³  ë¬¸ì„œ ë‚´ìš©ì„ í‘œì‹œ
        st.write(f"**ì§ˆë¬¸ ID**: {question_id}")
        st.write(f"**ì§ˆë¬¸ ë‚´ìš©**: {data['question']}")

        document_title = data["title"]
        document_content = data["context"]
        answer_text = data["answers"]["text"][0]
        answer_start = data["answers"]["answer_start"][0]
        answer_end = answer_start + len(answer_text)

        st.write(f"**ë¬¸ì„œ ID**: {data['document_id']}")
        st.write(f"**ë¬¸ì„œ ì œëª©**: {document_title}")

        # ë¬¸ì„œ ë‚´ìš©, ì •ë‹µ/ì˜ˆì¸¡ì— í•˜ì´ë¼ì´íŠ¸ í‘œì‹œ
        if question_id in prediction.keys():
            prediction_text = prediction[question_id][0]["text"]
            prediction_start = prediction[question_id][0]["start"]
            prediction_end = prediction_start + len(prediction_text)

            answer_prediction_span = ""
            for i in range(
                min(prediction_start, answer_start), max(prediction_end, answer_end)
            ):
                char = document_content[i]
                if answer_start <= i < answer_end:
                    char = f"<span style='background-color: #FD7E84;'>{char}</span>"
                if prediction_start <= i < prediction_end:
                    char = (
                        f"<span style='border-bottom: 3px solid #2E9AFE;'>{char}</span>"
                    )
                answer_prediction_span += char

            highlighted_content = (
                document_content[: min(answer_start, prediction_start)]
                + answer_prediction_span
                + document_content[max(answer_end, prediction_end) :]
            )
        else:
            highlighted_content = (
                document_content[:answer_start]
                + f"<span style='background-color: #FD7E84;'>{answer_text}</span>"
                + document_content[answer_start + len(answer_text) :]
            )
        st.markdown(highlighted_content, unsafe_allow_html=True)

        # ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µ/ì˜ˆì¸¡ í‘œì‹œ
        st.markdown(
            f"**ì •ë‹µ í…ìŠ¤íŠ¸**: <span style='background-color: #FD7E84;'>{answer_text}</span> (start: {answer_start})",
            unsafe_allow_html=True,
        )
        if question_id in prediction.keys():
            st.markdown(
                f"**ì˜ˆì¸¡ í…ìŠ¤íŠ¸**: <span style='border-bottom: 3px solid #2E9AFE;'>{prediction[question_id][0]['text']}</span> (start: {prediction[question_id][0]['start']})",
                unsafe_allow_html=True,
            )
            with st.expander("ì˜ˆì¸¡ nbest í™•ì¸"):
                for idx, pred in enumerate(prediction[question_id]):
                    st.write(f"{idx+1}: {pred['text']} (start: {pred['start']})")
    else:
        st.write("ì„ íƒëœ ë¬¸ì„œì™€ ì§ˆë¬¸ IDì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


@hydra.main(config_path="./config", config_name="mrc", version_base=None)
def main(config):
    """
    ì „ì²´ í™”ë©´ ìš”ì†Œë¥¼ ê·¸ë¦¬ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. (ì¶”í›„ ëª¨ë“ˆí™”, ì¬ì‚¬ìš©ì„± ë³´ì™„ ì˜ˆì •.)
    """
    # load data
    dataset = load_data(config)
    predictions = load_predictions(config)
    tokenized_samples = load_tokenized_samples(config)

    tab1, tab2, tab3, tab4 = st.tabs(
        ["í•˜ë‚˜ì”© ë³´ê¸°", "ë¬¶ìŒìœ¼ë¡œ ë³´ê¸°", "ìœ„í‚¤ ë¬¸ì„œ ì‚´í´ë³´ê¸°", "Retrieval ê²°ê³¼ ë³´ê¸°"]
    )
    # ================
    # í•˜ë‚˜ì”© ë³´ê¸° íƒ­
    # ================
    with tab1:
        setting_section, raw_data_section, analysis_section = st.columns([1, 2, 3])

        # ===============
        # setting_section
        # ===============
        with setting_section:
            st.header("í•„í„°ë§ ì˜µì…˜")
            document_options = ["0: ì „ì²´ ë³´ê¸°"]
            for _, row in dataset.iterrows():
                document_options.append(
                    ": ".join([str(row["document_id"]), row["title"]])
                )
            selected_document = st.selectbox(
                "ë¬¸ì„œ ì„ íƒ (0 = ì „ì²´ ë³´ê¸°)", document_options
            )
            selected_document_id = int(selected_document.split(":")[0])

            question_list = (
                dataset[dataset["document_id"] == selected_document_id]["id"]
                + ": "
                + dataset[dataset["document_id"] == selected_document_id]["question"]
                if selected_document_id
                else dataset["id"] + ": " + dataset["question"]
            )
            selected_question = st.selectbox("ì§ˆë¬¸ ì„ íƒ", question_list)
            selected_question_id = selected_question.split(":")[0]

            # ì„ íƒí•œ ë¬¸ì„œì™€ ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° í•„í„°ë§
            filtered_data = dataset[dataset["id"] == selected_question_id]

        # ================
        # raw_data_section
        # ================
        with raw_data_section:
            view_QA(selected_question_id, filtered_data.iloc[0], predictions)

        # ================
        # analysis_section
        # ================
        with analysis_section:
            if not filtered_data.empty:
                st.subheader("í† í°í™” ê²°ê³¼")

                if selected_question_id in tokenized_samples.keys():
                    colored_tokens = []
                    for token in tokenized_samples[selected_question_id]:
                        # ìŠ¤í˜ì…œ í† í° ì²˜ë¦¬
                        if token in ["[SEP]", "[CLS]"]:
                            colored_tokens.append(
                                f"<div style='display:inline-block; font-size:14px; background-color:#c5cff6; border:1px solid #ddd; border-radius:5px; padding:1px 5px; margin:2px;'>{token}</div>"
                            )
                        # íŒ¨ë“œ í† í°
                        elif token == "[PAD]":
                            colored_tokens.append(
                                f"<div style='display:inline-block; font-size:14px; color: #ddd; border:1px solid #ddd; border-radius:5px; padding:1px 5px; margin:2px;'>{token}</div>"
                            )
                        else:
                            colored_tokens.append(
                                f"<div style='display:inline-block; font-size:14px; border:1px solid #ddd; border-radius:5px; padding:1px 5px; margin:2px;'>{token}</div>"
                            )

                    st.markdown(" ".join(colored_tokens), unsafe_allow_html=True)
                else:
                    st.write("í•´ë‹¹ ì¿¼ë¦¬ì— ëŒ€í•œ í† í°í™” ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤ğŸ« ")

    # ================
    # ë¬¶ìŒìœ¼ë¡œ ë³´ê¸° íƒ­
    # ================
    with tab2:
        # í•œ í˜ì´ì§€ì— í‘œì‹œí•  í•­ëª© ê°œìˆ˜ ì„¤ì •
        items_per_page = st.selectbox(
            "í˜ì´ì§€ ë‹¹ í‘œì‹œí•  ì§ˆë¬¸ ìˆ˜", [5, 10, 15, 20], index=0
        )

        # ì „ì²´ í˜ì´ì§€ ìˆ˜ ê³„ì‚°
        total_pages = len(dataset) // items_per_page + (
            1 if len(dataset) % items_per_page > 0 else 0
        )

        # í˜„ì¬ í˜ì´ì§€ ì„ íƒ ìŠ¬ë¼ì´ë”
        current_page = st.slider("í˜ì´ì§€ ì„ íƒ", 1, total_pages, 1)

        # í˜„ì¬ í˜ì´ì§€ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ ì„ íƒ
        start_idx = (current_page - 1) * items_per_page
        end_idx = start_idx + items_per_page
        page_data = dataset.iloc[start_idx:end_idx]

        # í˜„ì¬ í˜ì´ì§€ì˜ ë°ì´í„° ì¶œë ¥
        st.write(f"### í˜„ì¬ í˜ì´ì§€: {current_page} / {total_pages}")

        for idx, row in page_data.iterrows():
            # ê° ì§ˆë¬¸-ë‹µë³€ í˜ì–´ í‘œì‹œ
            q_id = row["id"]
            with st.expander(f"{q_id} : {row['question']}"):
                col1, col2 = st.columns([2, 3])
                with col1:
                    if not row.empty:
                        # ì§ˆë¬¸, ë‹µë³€, ê·¸ë¦¬ê³  ë¬¸ì„œ ë‚´ìš©ì„ í‘œì‹œ
                        st.write(f"**ì§ˆë¬¸ ID**: {q_id}")
                        st.write(f"**ì§ˆë¬¸ ë‚´ìš©**: {row['question']}")

                        document_title = row["title"]
                        document_content = row["context"]
                        answer_text = row["answers"]["text"][0]
                        answer_start = row["answers"]["answer_start"][0]
                        answer_end = answer_start + len(answer_text)

                        st.write(f"**ë¬¸ì„œ ID**: {row['document_id']}")
                        st.write(f"**ë¬¸ì„œ ì œëª©**: {document_title}")

                        # ë¬¸ì„œ ë‚´ìš©, ì •ë‹µ/ì˜ˆì¸¡ì— í•˜ì´ë¼ì´íŠ¸ í‘œì‹œ
                        if q_id in predictions.keys():
                            prediction_text = predictions[q_id][0]["text"]
                            prediction_start = predictions[q_id][0]["start"]
                            prediction_end = prediction_start + len(prediction_text)

                            answer_prediction_span = ""
                            for i in range(
                                min(prediction_start, answer_start),
                                max(prediction_end, answer_end),
                            ):
                                char = document_content[i]
                                if answer_start <= i < answer_end:
                                    char = f"<span style='background-color: #FD7E84;'>{char}</span>"
                                if prediction_start <= i < prediction_end:
                                    char = f"<span style='border-bottom: 3px solid #2E9AFE;'>{char}</span>"
                                answer_prediction_span += char

                            highlighted_content = (
                                document_content[: min(answer_start, prediction_start)]
                                + answer_prediction_span
                                + document_content[max(answer_end, prediction_end) :]
                            )
                        else:
                            highlighted_content = (
                                document_content[:answer_start]
                                + f"<span style='background-color: #FD7E84;'>{answer_text}</span>"
                                + document_content[answer_start + len(answer_text) :]
                            )
                        st.markdown(highlighted_content, unsafe_allow_html=True)

                        # ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µ/ì˜ˆì¸¡ í‘œì‹œ
                        st.markdown(
                            f"**ì •ë‹µ í…ìŠ¤íŠ¸**: <span style='background-color: #FD7E84;'>{answer_text}</span> (start: {answer_start})",
                            unsafe_allow_html=True,
                        )
                        if q_id in predictions.keys():
                            st.markdown(
                                f"**ì˜ˆì¸¡ í…ìŠ¤íŠ¸**: <span style='border-bottom: 3px solid #2E9AFE;'>{predictions[q_id][0]['text']}</span> (start: {predictions[q_id][0]['start']})",
                                unsafe_allow_html=True,
                            )
                    else:
                        st.write("ì„ íƒëœ ë¬¸ì„œì™€ ì§ˆë¬¸ IDì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

                with col2:
                    if not row.empty:
                        st.subheader("í† í°í™” ê²°ê³¼")

                        if q_id in tokenized_samples.keys():
                            colored_tokens = []
                            for token in tokenized_samples[q_id]:
                                # ìŠ¤í˜ì…œ í† í° ì²˜ë¦¬
                                if token in ["[SEP]", "[CLS]"]:
                                    colored_tokens.append(
                                        f"<div style='display:inline-block; font-size:14px; background-color:#c5cff6; border:1px solid #ddd; border-radius:5px; padding:1px 5px; margin:2px;'>{token}</div>"
                                    )
                                # íŒ¨ë“œ í† í°
                                elif token == "[PAD]":
                                    colored_tokens.append(
                                        f"<div style='display:inline-block; font-size:14px; color: #ddd; border:1px solid #ddd; border-radius:5px; padding:1px 5px; margin:2px;'>{token}</div>"
                                    )
                                else:
                                    colored_tokens.append(
                                        f"<div style='display:inline-block; font-size:14px; border:1px solid #ddd; border-radius:5px; padding:1px 5px; margin:2px;'>{token}</div>"
                                    )

                            st.markdown(
                                " ".join(colored_tokens), unsafe_allow_html=True
                            )
                        else:
                            st.write("í•´ë‹¹ ì¿¼ë¦¬ì— ëŒ€í•œ í† í°í™” ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤ğŸ« ")

    # ================
    # ìœ„í‚¤ ë¬¸ì„œ ì‚´í´ë³´ê¸° íƒ­
    # ================
    with tab3:
        wiki_list = load_wiki()
        tfidf_infos = load_tfidf_info(config)

        # í•œ í˜ì´ì§€ì— í‘œì‹œí•  í•­ëª© ê°œìˆ˜ ì„¤ì •
        documents_per_page = st.selectbox(
            "í˜ì´ì§€ ë‹¹ í‘œì‹œí•  ë¬¸ì„œ ìˆ˜", [10, 20, 30], index=0
        )

        # ì „ì²´ í˜ì´ì§€ ìˆ˜ ê³„ì‚°
        total_document_pages = len(wiki_list) // documents_per_page + (
            1 if len(wiki_list) % documents_per_page > 0 else 0
        )

        # í˜„ì¬ í˜ì´ì§€ ì„ íƒ ìŠ¬ë¼ì´ë”
        document_page = st.slider(
            "í˜ì´ì§€ ì„ íƒ", 1, total_document_pages, 1, key="document_slider"
        )

        # í˜„ì¬ í˜ì´ì§€ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ ì„ íƒ
        start_idx = (document_page - 1) * documents_per_page
        end_idx = start_idx + documents_per_page
        selected_documents = wiki_list[start_idx:end_idx]

        # í˜„ì¬ í˜ì´ì§€ì˜ ë°ì´í„° ì¶œë ¥
        st.write(f"### í˜„ì¬ í˜ì´ì§€: {document_page} / {total_document_pages}")

        for idx, wiki_document in enumerate(selected_documents):
            tfidf_info = tfidf_infos[str(start_idx + idx)]
            with st.expander(
                f"{wiki_document['document_id']}: {wiki_document['title']}"
            ):
                st.write(
                    f"Domain: {wiki_document['domain']} | Corpus Source: {wiki_document['corpus_source']} | Author: {wiki_document['author']} html: {wiki_document['html']} | url: {wiki_document['url']}"
                )

                c1, c2 = st.columns(2)
                with c1:
                    st.subheader("Markdown")
                    st.write(wiki_document["text"])
                with c2:
                    st.subheader("Raw text")
                    st.text(wiki_document["text"])

                st.subheader("TF-IDF ì ìˆ˜ ìƒìœ„ 10ê°œ í† í°")
                tfidf_text = []
                for t, score in tfidf_info.items():
                    tfidf_text.append(
                        f"<div style='display:inline-block; font-size:14px; background-color:#c5cff6; border:1px solid #ddd; border-radius:5px; padding:1px 5px; margin:2px;'>{t}</div>: {round(score, 3)}"
                    )
                st.markdown(", ".join(tfidf_text), unsafe_allow_html=True)

    with tab4:
        test_dataset, retrieved_documents = load_retrieved_documents(config)


if __name__ == "__main__":
    main()
